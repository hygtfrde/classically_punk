{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb17f9d-19dd-41c1-a6de-37f734b9e54a",
   "metadata": {},
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9efc3cc-5ea1-4467-a381-911e180b067d",
   "metadata": {},
   "source": [
    "# Download and set path to music data\n",
    "https://storage.googleapis.com/qwasar-public/track-ds/classically_punk_music_genres.tar.gz \n",
    "## The data will be in a 'genres' directory. Use it in the Music Processor code\n",
    "dataset_path = '../genres'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a7d5b-3eac-4c85-83ce-a8e68d487548",
   "metadata": {},
   "source": [
    "# Import statements for rest of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ca2e061-c643-4ef2-842e-098eed5bf0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import importlib.util\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# TENSORFLOW IS REQUIRED EVEN IF NOT ACCESSED\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce5edd0-32d9-4513-9b3b-5b8b7978dd55",
   "metadata": {},
   "source": [
    "# Helpers and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f823a3-f7e6-4d0e-9b54-a90b6615bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREEN = '\\033[32m'\n",
    "RED = '\\033[31m'\n",
    "RESET = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70153522-2c2c-4b69-a1ff-7acf864a3d63",
   "metadata": {},
   "source": [
    "# MUSIC PROCESSOR CODE\n",
    "## Used to extract audio feature data from the genres dataset\n",
    "## Skip to Model Training code if data is already processed in `df_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af794bb8-f23e-41a5-8dd2-e787e9ed71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "genres_from_dataset = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "fundamental_features_cols = [\n",
    "    'mfcc', 'chroma', 'mel', 'contrast', 'tonnetz'\n",
    "]\n",
    "\n",
    "df_output_dir = 'df_output'\n",
    "\n",
    "class MusicDataProcessor:\n",
    "    def __init__(\n",
    "            self, \n",
    "            dataset_path: str, \n",
    "            file_depth_limit: int, \n",
    "            file_output_name: str, \n",
    "            extract_raw_only: bool,\n",
    "            compute_kde: bool,\n",
    "            compute_ecdf: bool,\n",
    "            pad_and_truncate: bool\n",
    "        ):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.file_depth_limit = file_depth_limit\n",
    "        self.file_output_name = file_output_name\n",
    "        self.genres = genres_from_dataset\n",
    "        self.data = pd.DataFrame(columns=fundamental_features_cols)\n",
    "        self.extract_raw_only = extract_raw_only\n",
    "        self.compute_kde = compute_kde\n",
    "        self.compute_ecdf = compute_ecdf\n",
    "        self.pad_and_truncate = pad_and_truncate\n",
    "\n",
    "        if not os.path.exists(df_output_dir):\n",
    "            os.makedirs(df_output_dir)\n",
    "            print(f\"Directory '{df_output_dir}' created.\")\n",
    "        else:\n",
    "            print(f\"Directory '{df_output_dir}' already exists.\")\n",
    "\n",
    "    def get_data(self):\n",
    "        def encode_array(x):\n",
    "            if isinstance(x, np.ndarray):\n",
    "                # Convert the array to a JSON string\n",
    "                return json.dumps(x.tolist())\n",
    "            return x\n",
    "        encoded_df = self.data.map(encode_array)\n",
    "        encoded_df.to_csv(f'{df_output_dir}/{self.file_output_name}.csv', index=False)\n",
    "        return encoded_df\n",
    "\n",
    "    def compute_stats_and_measures(self, data):\n",
    "        # Compute basic statistics\n",
    "        stats_dict = {\n",
    "            'mean': np.mean(data),\n",
    "            'stddev': np.std(data),\n",
    "            'var': np.var(data),\n",
    "            'min': np.min(data),\n",
    "            'max': np.max(data),\n",
    "            'mad': stats.median_abs_deviation(data),\n",
    "            'kurtosis': kurtosis(data),\n",
    "            'skewness': skew(data)\n",
    "        }\n",
    "        \n",
    "        # Compute ECDF\n",
    "        if self.compute_ecdf:\n",
    "            sorted_data, ecdf = np.sort(data), np.arange(1, len(data) + 1) / len(data)\n",
    "            stats_dict['ecdf_values'] = sorted_data.tolist()\n",
    "            stats_dict['ecdf_proportions'] = ecdf.tolist()\n",
    "        \n",
    "        # Compute KDE\n",
    "        if self.compute_kde:\n",
    "            kde = stats.gaussian_kde(data)\n",
    "            stats_dict['kde'] = kde\n",
    "        \n",
    "        return stats_dict\n",
    "\n",
    "    def extract_features(self, file_path, verbose=None):\n",
    "        try:\n",
    "            target_rows = 13\n",
    "            target_columns = 1293\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "            n_fft = min(1024, len(y))\n",
    "            \n",
    "            def pad_or_truncate(feature, target_columns):\n",
    "                # Truncate\n",
    "                if feature.shape[1] > target_columns:\n",
    "                    return feature[:, :target_columns]\n",
    "                # Pad\n",
    "                elif feature.shape[1] < target_columns:\n",
    "                    pad_width = target_columns - feature.shape[1]\n",
    "                    return np.pad(feature, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                return feature\n",
    "\n",
    "            features = {\n",
    "                'mfcc': librosa.feature.mfcc(y=y, sr=sr, n_mfcc=target_rows, n_fft=n_fft),\n",
    "                'chroma': librosa.feature.chroma_stft(y=y, sr=sr, hop_length=n_fft // 4),\n",
    "                'mel': librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft),\n",
    "                'contrast': librosa.feature.spectral_contrast(y=y, sr=sr, n_fft=n_fft),\n",
    "                'tonnetz': librosa.feature.tonnetz(y=y, sr=sr),\n",
    "                'spectral_bandwidth': librosa.feature.spectral_bandwidth(y=y, sr=sr, n_fft=n_fft),\n",
    "                'spectral_flatness': librosa.feature.spectral_flatness(y=y),\n",
    "                'spectral_centroid': librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=n_fft),\n",
    "                'zero_crossing_rate': librosa.feature.zero_crossing_rate(y=y),\n",
    "                'harmony': librosa.effects.harmonic(y).reshape(1, -1),  # Reshape to 2D array\n",
    "                'perceptr': librosa.effects.percussive(y).reshape(1, -1),  # Reshape to 2D array\n",
    "                'tempo': np.array([librosa.beat.beat_track(y=y, sr=sr)[0]]).reshape(1, 1),  # Ensure shape compatibility\n",
    "                'spectral_rolloff': librosa.feature.spectral_rolloff(y=y, sr=sr, n_fft=n_fft),\n",
    "                'rms': librosa.feature.rms(y=y, frame_length=n_fft)\n",
    "            }\n",
    "            \n",
    "            if self.pad_and_truncate:\n",
    "                for key in features:\n",
    "                    if len(features[key].shape) == 2:\n",
    "                        features[key] = pad_or_truncate(features[key], target_columns)\n",
    "                    else:\n",
    "                        # Handle 1D features (e.g., tempo, harmony)\n",
    "                        features[key] = pad_or_truncate(features[key].reshape(1, -1), target_columns)\n",
    "\n",
    "            \n",
    "            if self.extract_raw_only is not None and self.extract_raw_only:\n",
    "                if verbose == 'v':\n",
    "                    for name, array in features.items():\n",
    "                        print(f\"{name.capitalize()} Shape: {array.shape}\")\n",
    "                return features\n",
    "\n",
    "            # Compute statistics for each feature\n",
    "            feature_stats = {}\n",
    "            for feature_name, feature_array in features.items():\n",
    "                if feature_array.ndim == 1:  # If the feature is 1D\n",
    "                    feature_stats.update({\n",
    "                        f'{feature_name}_mean': np.mean(feature_array),\n",
    "                        f'{feature_name}_stddev': np.std(feature_array),\n",
    "                        f'{feature_name}_var': np.var(feature_array),\n",
    "                        f'{feature_name}_min': np.min(feature_array),\n",
    "                        f'{feature_name}_max': np.max(feature_array)\n",
    "                    })\n",
    "                else:  # If the feature is 2D\n",
    "                    num_features = feature_array.shape[0]\n",
    "                    for i in range(num_features):\n",
    "                        feature_i = feature_array[i, :]\n",
    "                        feature_stats.update({\n",
    "                            f'{feature_name}_{i+1}_{key}': value\n",
    "                            for key, value in self.compute_stats_and_measures(feature_i).items()\n",
    "                        })\n",
    "\n",
    "            if verbose == 'v':\n",
    "                for key, value in feature_stats.items():\n",
    "                    print(f\"EXTRACTING: {key}\\n{value}\")\n",
    "\n",
    "            return feature_stats\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        all_data = []\n",
    "        total_files_counter = 0\n",
    "        for genre in self.genres:\n",
    "            counter = 0\n",
    "            genre_dir = os.path.join(self.dataset_path, genre)\n",
    "            for file in os.listdir(genre_dir):\n",
    "                # print(f'File number: {total_files_counter}')\n",
    "                if self.file_depth_limit and counter >= self.file_depth_limit:\n",
    "                    break\n",
    "                file_path = os.path.join(genre_dir, file)\n",
    "                features = self.extract_features(file_path, None)\n",
    "                if features:\n",
    "                    # Flatten and unpack the data structure\n",
    "                    stats_flat = features\n",
    "                    all_data.append({\n",
    "                        'filename': file,\n",
    "                        'genre': genre,\n",
    "                        **stats_flat\n",
    "                    })                                      \n",
    "                    counter += 1\n",
    "                    total_files_counter += 1\n",
    "\n",
    "        self.data = pd.DataFrame(all_data)\n",
    "        self.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf079f3-4079-432f-9268-4df04e56d5b8",
   "metadata": {},
   "source": [
    "# Run Music Processor to Extract Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59e38a-800e-4132-8877-b005d81bb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "dataset_path = '../genres'  # Replace with the path to your audio dataset\n",
    "file_depth_limit = None  # Number of files to process per genre\n",
    "file_output_name = 'full_audio_features'\n",
    "\n",
    "# Create an instance of the MusicDataProcessor\n",
    "processor = MusicDataProcessor(\n",
    "    dataset_path=dataset_path,\n",
    "    file_output_name=file_output_name, \n",
    "    file_depth_limit=file_depth_limit,\n",
    "    extract_raw_only=True,\n",
    "    pad_and_truncate=True,\n",
    "    compute_kde=False,\n",
    "    compute_ecdf=False\n",
    ")\n",
    "\n",
    "# Load data\n",
    "processor.load_data()\n",
    "\n",
    "# Output the processed data\n",
    "print(f\"Data has been processed and saved to CSV file: {file_output_name}.\")\n",
    "print(processor.data.head())  # Display the first few rows of the processed data\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = int(elapsed_time % 60)\n",
    "\n",
    "print(f\"Time taken: {minutes} minutes and {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41880075-d725-40a7-9c3b-7847fa46f5be",
   "metadata": {},
   "source": [
    "# Model Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2747cb22-d9e2-4a13-b303-fffc38e97128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_array(value):\n",
    "    try: \n",
    "        if isinstance(value, str):\n",
    "            value = value.strip('\"').strip(\"'\")\n",
    "            try:\n",
    "                value = ast.literal_eval(value)\n",
    "                if isinstance(value, list):\n",
    "                    value = np.array(value, dtype=float)\n",
    "                    return value\n",
    "                else:\n",
    "                    print(\"Warning: Evaluated value is not a list.\")\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(f\"Error evaluating string: {e}\")\n",
    "        else:\n",
    "            print('Value not detected as str')\n",
    "        return value\n",
    "    except Exception as e:\n",
    "        print(\"General failure in conversion:\")\n",
    "        print(f'Error: {e}')\n",
    "        return value\n",
    "\n",
    "def read_raw_str_csv_and_split_df(csv_path):\n",
    "    try:\n",
    "        df_input = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading csv into df: {e}\")\n",
    "        return None, None\n",
    "    if df_input is not None:\n",
    "        for col in df_input.columns:\n",
    "            if col not in ['filename', 'genre']:\n",
    "                df_input[col] = df_input[col].apply(convert_string_to_array)\n",
    "        return df_input\n",
    "    else:\n",
    "        print('Error: df_input is None')\n",
    "        return None, None\n",
    "\n",
    "def prepare_data(X, y):\n",
    "    try:\n",
    "        # Step 1: Flatten the features\n",
    "        X_flattened = X.apply(lambda col: col.apply(lambda x: x.flatten()))\n",
    "        # Step 2: Convert the DataFrame of flattened arrays into a 2D NumPy array\n",
    "        X_stacked = np.stack(X_flattened.apply(lambda x: np.concatenate(x), axis=1).to_numpy())\n",
    "        # Step 3: Scale the features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_stacked)\n",
    "        # Step 4: Encode the target labels (y)\n",
    "        encoder = LabelEncoder()\n",
    "        y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "        return X_scaled, y_encoded, encoder, scaler\n",
    "    except Exception as e:\n",
    "        print(f\"Error in prepare_data: {e}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "\n",
    "\n",
    "def build_and_train_model(X_train, y_train, X_test, y_test, num_features, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=(num_features,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        epochs=3000, \n",
    "        batch_size=128, \n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def predict(model, encoder, scaler, feature_inputs):\n",
    "    # Scale the feature inputs directly without converting to DataFrame\n",
    "    feature_inputs_scaled = scaler.transform([feature_inputs])\n",
    "    # Make predictions\n",
    "    predictions = model.predict(feature_inputs_scaled)\n",
    "    # Decode the predictions to category names\n",
    "    predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "    predicted_class = encoder.inverse_transform([predicted_class_index])[0]\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "\n",
    "def evaluate_all_rows(model, X, y, encoder, scaler):\n",
    "    correct_count = 0\n",
    "    total_count = len(X)\n",
    "    \n",
    "    for i in range(total_count):\n",
    "        # Extract feature inputs and true label\n",
    "        feature_inputs = X[i]  # Use standard NumPy indexing\n",
    "        true_label = y[i]  # Use standard NumPy indexing\n",
    "        # Make prediction\n",
    "        predicted_class = predict(model, encoder, scaler, feature_inputs)\n",
    "        # Check if the prediction matches the true label\n",
    "        if predicted_class == true_label:\n",
    "            # print(f\"{GREEN}TRUE: {predicted_class} is {true_label}{RESET}\")\n",
    "            correct_count += 1\n",
    "        # else:\n",
    "            # print(f\"{RED}FALSE: {predicted_class} is NOT {true_label}{RESET}\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = (correct_count / total_count) * 100\n",
    "    incorrect_count = total_count - correct_count\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e473f7-c9bf-470a-ad66-2e12e4ba9664",
   "metadata": {},
   "source": [
    "# Run Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5de27fb-b77b-457d-8525-4fabf65fa64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rx/tnwbq8qs0ybbs61jm_vxtjmw0000gn/T/ipykernel_96084/408384716.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  X_stacked = np.stack(X_flattened.apply(lambda x: np.concatenate(x), axis=1).to_numpy())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Accuracy: 80.00%\n"
     ]
    }
   ],
   "source": [
    "full_dataset_stable = '../df_output/v5_5.csv'\n",
    "\n",
    "try:\n",
    "    df_extract = read_raw_str_csv_and_split_df(full_dataset_stable)\n",
    "    \n",
    "    if df_extract is not None:\n",
    "        # Split into X and y\n",
    "        X = df_extract.drop(columns=['filename', 'genre'])\n",
    "        y = df_extract['genre']\n",
    "        categories = y.unique()\n",
    "        num_classes = len(categories)\n",
    "\n",
    "        # Prepare the data\n",
    "        X_scaled, y_encoded, encoder, scaler = prepare_data(X, y)\n",
    "        y_encoded_one_hot = to_categorical(y_encoded, num_classes=num_classes)\n",
    "        if X_scaled is not None and y_encoded is not None:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded_one_hot, test_size=0.2, random_state=42)\n",
    "        else:\n",
    "            print(\"Error in data preparation\")\n",
    "            raise ValueError(\"X_scaled or y_encoded is None\")\n",
    "    \n",
    "        model, history = build_and_train_model(X_train, y_train, X_test, y_test, X_scaled.shape[1], num_classes) \n",
    "        \n",
    "        # Evaluate model\n",
    "        evaluate_all_rows(model, X_scaled, y, encoder, scaler)\n",
    "    else:\n",
    "        print(\"Error: DataFrame is None\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"A general error occurred in main block: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11ab40-a1b9-4fe9-ab3f-0e1d7f0a9421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
