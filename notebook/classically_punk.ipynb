{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb17f9d-19dd-41c1-a6de-37f734b9e54a",
   "metadata": {},
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a7d5b-3eac-4c85-83ce-a8e68d487548",
   "metadata": {},
   "source": [
    "# Import statements for rest of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ca2e061-c643-4ef2-842e-098eed5bf0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import ast\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "# TENSORFLOW IS REQUIRED (by Keras) EVEN IF NOT ACCESSED\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from CONSTANTS import RED, GREEN, RESET\n",
    "from helpers import get_input_with_timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70153522-2c2c-4b69-a1ff-7acf864a3d63",
   "metadata": {},
   "source": [
    "# MUSIC PROCESSOR CODE\n",
    "## Used to extract audio feature data from the genres dataset\n",
    "## Skip to Model Training code if data is already processed in `df_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af794bb8-f23e-41a5-8dd2-e787e9ed71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "genres_from_dataset = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "fundamental_features_cols = [\n",
    "    'mfcc', 'chroma', 'mel', 'contrast', 'tonnetz'\n",
    "]\n",
    "\n",
    "df_output_dir = 'df_output'\n",
    "\n",
    "class MusicDataProcessor:\n",
    "    def __init__(\n",
    "            self, \n",
    "            dataset_path: str, \n",
    "            file_depth_limit: int, \n",
    "            file_output_name: str, \n",
    "            extract_raw_only: bool,\n",
    "            compute_kde: bool,\n",
    "            compute_ecdf: bool,\n",
    "            pad_and_truncate: bool\n",
    "        ):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.file_depth_limit = file_depth_limit\n",
    "        self.file_output_name = file_output_name\n",
    "        self.genres = genres_from_dataset\n",
    "        self.data = pd.DataFrame(columns=fundamental_features_cols)\n",
    "        self.extract_raw_only = extract_raw_only\n",
    "        self.compute_kde = compute_kde\n",
    "        self.compute_ecdf = compute_ecdf\n",
    "        self.pad_and_truncate = pad_and_truncate\n",
    "\n",
    "        if not os.path.exists(df_output_dir):\n",
    "            os.makedirs(df_output_dir)\n",
    "            print(f\"Directory '{df_output_dir}' created.\")\n",
    "        else:\n",
    "            print(f\"Directory '{df_output_dir}' already exists.\")\n",
    "\n",
    "    def get_data(self):\n",
    "        def encode_array(x):\n",
    "            if isinstance(x, np.ndarray):\n",
    "                # Convert the array to a JSON string\n",
    "                return json.dumps(x.tolist())\n",
    "            return x\n",
    "        encoded_df = self.data.map(encode_array)\n",
    "        encoded_df.to_csv(f'{df_output_dir}/{self.file_output_name}.csv', index=False)\n",
    "        return encoded_df\n",
    "\n",
    "    def compute_stats_and_measures(self, data):\n",
    "        # Compute basic statistics\n",
    "        stats_dict = {\n",
    "            'mean': np.mean(data),\n",
    "            'stddev': np.std(data),\n",
    "            'var': np.var(data),\n",
    "            'min': np.min(data),\n",
    "            'max': np.max(data),\n",
    "            'mad': stats.median_abs_deviation(data),\n",
    "            'kurtosis': kurtosis(data),\n",
    "            'skewness': skew(data)\n",
    "        }\n",
    "        \n",
    "        # Compute ECDF\n",
    "        if self.compute_ecdf:\n",
    "            sorted_data, ecdf = np.sort(data), np.arange(1, len(data) + 1) / len(data)\n",
    "            stats_dict['ecdf_values'] = sorted_data.tolist()\n",
    "            stats_dict['ecdf_proportions'] = ecdf.tolist()\n",
    "        \n",
    "        # Compute KDE\n",
    "        if self.compute_kde:\n",
    "            kde = stats.gaussian_kde(data)\n",
    "            stats_dict['kde'] = kde\n",
    "        \n",
    "        return stats_dict\n",
    "\n",
    "    def extract_features(self, file_path, verbose=None):\n",
    "        try:\n",
    "            target_rows = 13\n",
    "            target_columns = 1293\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "            n_fft = min(1024, len(y))\n",
    "            \n",
    "            def pad_or_truncate(feature, target_columns):\n",
    "                # Truncate\n",
    "                if feature.shape[1] > target_columns:\n",
    "                    return feature[:, :target_columns]\n",
    "                # Pad\n",
    "                elif feature.shape[1] < target_columns:\n",
    "                    pad_width = target_columns - feature.shape[1]\n",
    "                    return np.pad(feature, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                return feature\n",
    "\n",
    "            features = {\n",
    "                'mfcc': librosa.feature.mfcc(y=y, sr=sr, n_mfcc=target_rows, n_fft=n_fft),\n",
    "                'chroma': librosa.feature.chroma_stft(y=y, sr=sr, hop_length=n_fft // 4),\n",
    "                'mel': librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft),\n",
    "                'contrast': librosa.feature.spectral_contrast(y=y, sr=sr, n_fft=n_fft),\n",
    "                'tonnetz': librosa.feature.tonnetz(y=y, sr=sr),\n",
    "                'spectral_bandwidth': librosa.feature.spectral_bandwidth(y=y, sr=sr, n_fft=n_fft),\n",
    "                'spectral_flatness': librosa.feature.spectral_flatness(y=y),\n",
    "                'spectral_centroid': librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=n_fft),\n",
    "                'zero_crossing_rate': librosa.feature.zero_crossing_rate(y=y),\n",
    "                'harmony': librosa.effects.harmonic(y).reshape(1, -1),  # Reshape to 2D array\n",
    "                'perceptr': librosa.effects.percussive(y).reshape(1, -1),  # Reshape to 2D array\n",
    "                'tempo': np.array([librosa.beat.beat_track(y=y, sr=sr)[0]]).reshape(1, 1),  # Ensure shape compatibility\n",
    "                'spectral_rolloff': librosa.feature.spectral_rolloff(y=y, sr=sr, n_fft=n_fft),\n",
    "                'rms': librosa.feature.rms(y=y, frame_length=n_fft)\n",
    "            }\n",
    "            \n",
    "            if self.pad_and_truncate:\n",
    "                for key in features:\n",
    "                    if len(features[key].shape) == 2:\n",
    "                        features[key] = pad_or_truncate(features[key], target_columns)\n",
    "                    else:\n",
    "                        # Handle 1D features (e.g., tempo, harmony)\n",
    "                        features[key] = pad_or_truncate(features[key].reshape(1, -1), target_columns)\n",
    "\n",
    "            \n",
    "            if self.extract_raw_only is not None and self.extract_raw_only:\n",
    "                if verbose == 'v':\n",
    "                    for name, array in features.items():\n",
    "                        print(f\"{name.capitalize()} Shape: {array.shape}\")\n",
    "                return features\n",
    "\n",
    "            # Compute statistics for each feature\n",
    "            feature_stats = {}\n",
    "            for feature_name, feature_array in features.items():\n",
    "                if feature_array.ndim == 1:  # If the feature is 1D\n",
    "                    feature_stats.update({\n",
    "                        f'{feature_name}_mean': np.mean(feature_array),\n",
    "                        f'{feature_name}_stddev': np.std(feature_array),\n",
    "                        f'{feature_name}_var': np.var(feature_array),\n",
    "                        f'{feature_name}_min': np.min(feature_array),\n",
    "                        f'{feature_name}_max': np.max(feature_array)\n",
    "                    })\n",
    "                else:  # If the feature is 2D\n",
    "                    num_features = feature_array.shape[0]\n",
    "                    for i in range(num_features):\n",
    "                        feature_i = feature_array[i, :]\n",
    "                        feature_stats.update({\n",
    "                            f'{feature_name}_{i+1}_{key}': value\n",
    "                            for key, value in self.compute_stats_and_measures(feature_i).items()\n",
    "                        })\n",
    "\n",
    "            if verbose == 'v':\n",
    "                for key, value in feature_stats.items():\n",
    "                    print(f\"EXTRACTING: {key}\\n{value}\")\n",
    "\n",
    "            return feature_stats\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        all_data = []\n",
    "        total_files_counter = 0\n",
    "        for genre in self.genres:\n",
    "            counter = 0\n",
    "            genre_dir = os.path.join(self.dataset_path, genre)\n",
    "            for file in os.listdir(genre_dir):\n",
    "                # print(f'File number: {total_files_counter}')\n",
    "                if self.file_depth_limit and counter >= self.file_depth_limit:\n",
    "                    break\n",
    "                file_path = os.path.join(genre_dir, file)\n",
    "                features = self.extract_features(file_path, None)\n",
    "                if features:\n",
    "                    # Flatten and unpack the data structure\n",
    "                    stats_flat = features\n",
    "                    all_data.append({\n",
    "                        'filename': file,\n",
    "                        'genre': genre,\n",
    "                        **stats_flat\n",
    "                    })                                      \n",
    "                    counter += 1\n",
    "                    total_files_counter += 1\n",
    "\n",
    "        self.data = pd.DataFrame(all_data)\n",
    "        self.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf079f3-4079-432f-9268-4df04e56d5b8",
   "metadata": {},
   "source": [
    "# Run Music Processor to Extract Audio Feature Data from song files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59e38a-800e-4132-8877-b005d81bb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "dataset_path = '../genres'  # Replace with the path to your audio dataset\n",
    "file_depth_limit = None  # Number of files to process per genre\n",
    "file_output_name = 'full_audio_features'\n",
    "\n",
    "# Create an instance of the MusicDataProcessor\n",
    "processor = MusicDataProcessor(\n",
    "    dataset_path=dataset_path,\n",
    "    file_output_name=file_output_name, \n",
    "    file_depth_limit=file_depth_limit,\n",
    "    extract_raw_only=True,\n",
    "    pad_and_truncate=True,\n",
    "    compute_kde=False,\n",
    "    compute_ecdf=False\n",
    ")\n",
    "\n",
    "# Load data\n",
    "processor.load_data()\n",
    "\n",
    "# Output the processed data\n",
    "print(f\"Data has been processed and saved to CSV file: {file_output_name}.\")\n",
    "print(processor.data.head())  # Display the first few rows of the processed data\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = int(elapsed_time % 60)\n",
    "\n",
    "print(f\"Time taken: {minutes} minutes and {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41880075-d725-40a7-9c3b-7847fa46f5be",
   "metadata": {},
   "source": [
    "# Model Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2747cb22-d9e2-4a13-b303-fffc38e97128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_array(value):\n",
    "    try: \n",
    "        if isinstance(value, str):\n",
    "            value = value.strip('\"').strip(\"'\")\n",
    "            try:\n",
    "                value = ast.literal_eval(value)\n",
    "                if isinstance(value, list):\n",
    "                    value = np.array(value, dtype=float)\n",
    "                    return value\n",
    "                else:\n",
    "                    print(\"Warning: Evaluated value is not a list.\")\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(f\"Error evaluating string: {e}\")\n",
    "        else:\n",
    "            print('Value not detected as str')\n",
    "        return value\n",
    "    except Exception as e:\n",
    "        print(\"General failure in conversion:\")\n",
    "        print(f'Error: {e}')\n",
    "        return value\n",
    "\n",
    "def read_raw_str_csv_and_split_df(csv_path):\n",
    "    try:\n",
    "        df_input = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading csv into df: {e}\")\n",
    "        return None, None\n",
    "    if df_input is not None:\n",
    "        for col in df_input.columns:\n",
    "            if col not in ['filename', 'genre', 'harmony', 'perceptr', 'tempo']:\n",
    "                df_input[col] = df_input[col].apply(convert_string_to_array)\n",
    "        return df_input\n",
    "    else:\n",
    "        print('Error: df_input is None')\n",
    "        return None, None\n",
    "\n",
    "def prepare_data(X, y):\n",
    "    try:\n",
    "        # Step 1: Flatten the features\n",
    "        X_flattened = X.apply(lambda col: col.apply(lambda x: x.flatten()))\n",
    "        # Step 2: Convert the DataFrame of flattened arrays into a 2D NumPy array\n",
    "        X_stacked = np.stack(X_flattened.apply(np.concatenate, axis=1).to_numpy())\n",
    "        # Step 3: Scale the features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_stacked)\n",
    "        # Step 4: Encode the target labels (y)\n",
    "        encoder = LabelEncoder()\n",
    "        y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "        return X_scaled, y_encoded, encoder, scaler\n",
    "    except Exception as e:\n",
    "        print(f\"Error in prepare_data: {e}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "\n",
    "\n",
    "def build_and_train_model(X_train, y_train, X_test, y_test, num_features, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=(num_features,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    class EpochLogger(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch + 1}: loss = {logs['loss']:.4f}, accuracy = {logs['accuracy']:.4f}\")\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        epochs=300, \n",
    "        batch_size=128, \n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1,\n",
    "        callbacks=[EpochLogger()]\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e473f7-c9bf-470a-ad66-2e12e4ba9664",
   "metadata": {},
   "source": [
    "# Run Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5de27fb-b77b-457d-8525-4fabf65fa64a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA general error occurred in main block: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m full_dataset_stable \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_output/full_audio_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     df_extract \u001b[38;5;241m=\u001b[39m \u001b[43mread_raw_str_csv_and_split_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_dataset_stable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df_extract \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# Split into X and y\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         X \u001b[38;5;241m=\u001b[39m df_extract\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mharmony\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperceptr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtempo\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[24], line 36\u001b[0m, in \u001b[0;36mread_raw_str_csv_and_split_df\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_input\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mharmony\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperceptr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtempo\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 36\u001b[0m             df_input[col] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_string_to_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_input\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m, in \u001b[0;36mconvert_string_to_array\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m      4\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m      9\u001b[0m         value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ast.py:66\u001b[0m, in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03mEvaluate an expression node or a string containing only a Python\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mexpression.  The string or node provided may only consist of the following\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03mCaution: A complex expression can overflow the C stack and cause a crash.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node_or_string, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 66\u001b[0m     node_or_string \u001b[38;5;241m=\u001b[39m \u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node_or_string, Expression):\n\u001b[1;32m     68\u001b[0m     node_or_string \u001b[38;5;241m=\u001b[39m node_or_string\u001b[38;5;241m.\u001b[39mbody\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ast.py:52\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(source, filename, mode, type_comments, feature_version)\u001b[0m\n\u001b[1;32m     50\u001b[0m     feature_version \u001b[38;5;241m=\u001b[39m minor\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Else it should be an int giving the minor version for 3.x.\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m               \u001b[49m\u001b[43m_feature_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_version\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "full_dataset_stable = 'df_output/full_audio_features.csv'\n",
    "\n",
    "try:\n",
    "    df_extract = read_raw_str_csv_and_split_df(full_dataset_stable)\n",
    "    \n",
    "    if df_extract is not None:\n",
    "        # Split into X and y\n",
    "        X = df_extract.drop(columns=['filename', 'genre', 'harmony', 'perceptr', 'tempo'])\n",
    "        y = df_extract['genre']\n",
    "        categories = y.unique()\n",
    "        num_classes = len(categories)\n",
    "\n",
    "        # Prepare the data\n",
    "        X_scaled, y_encoded, encoder, scaler = prepare_data(X, y)\n",
    "        y_encoded_one_hot = to_categorical(y_encoded, num_classes=num_classes)\n",
    "        if X_scaled is not None and y_encoded is not None:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded_one_hot, test_size=0.2, random_state=42)\n",
    "        else:\n",
    "            print(\"Error in data preparation\")\n",
    "            raise ValueError(\"X_scaled or y_encoded is None\")\n",
    "    \n",
    "        model, history = build_and_train_model(X_train, y_train, X_test, y_test, X_scaled.shape[1], num_classes) \n",
    "    else:\n",
    "        print(\"Error: DataFrame is None\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"A general error occurred in main block: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11ab40-a1b9-4fe9-ab3f-0e1d7f0a9421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
